\documentclass[12pt]{amsart}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{algorithmic}

\newcommand{\R}{\mathbb{R}}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}

\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newtheorem{defn}{Definition}


\title{Notes on sparse spectral method}
\date{Sep 25, 2010}


\begin{document}
\maketitle

\section{Introduction}

The spectral method is a fundamental tool for determining eigenstates of Hamiltonian systems. Key to the method is an FFT of the autocorrelation function, $\mathcal{P}(t)$, of a time dependent wavefunction into the energy domain where relevant spectral information may be obtained via line fitting techniques. Reliable determination of the eigenstates thus depends on knowledge of the autocorrelation function at a high sample rate over a long time.

The accuracy of the energy domain representation limited by the uncertainty principle in two ways. Highly resolved energy domain signals require long time propagations. Explicitly, the lower bound on the density of states that can be resolved is $\frac{2 \pi}{T}$ where $T$ is the total propagation time. In the situation where one desires eigenvalues in a specified range this uncertainty principle has been overcome by Neuhauser with the Filter Diagonalization method \cite{Wall1995} \cite{Neuhauser1994} \cite{Mandelshtam2001}.

On the other hand the autocorrelation function must be sampled at the Nyquist rate $ \Delta t < \pi / \Delta E $ to avoid aliasing errors. This is traditionally dealt with by truncating the potential function appropriately and advancing the time dependent wave function over short time steps. In this work we overcome this restriction by exploiting the spare structure of $\hat{\mathcal{P}}(\lambda)$ and making use of ideas from the theory of compressed sensing \cite{Candes2006}.

Specifically, we propose that the energy domain representation of the autocorrelation function
\begin{equation}
\hat{R}(\lambda) = \sum_n^d |A_n|^2 \delta(\lambda-\lambda_n)
\end{equation}
be modeled as a sparse signal which implies that it may be recovered from sparse random measurements of $R(t)$. In particular, our results guarantee that the eigenvalues may be exactly obtained with overwhelmingly high probability provided that the number of samples of the autocorrelation function is of cardinality $\mathcal{O}(d \log(N))$ where $N$ is the number of points in our arrays and $d$ is the rank of our operator.

\section{Review of the spectral method}

We are interested in solutions to
\begin{equation}
H \psi = \lambda \psi
\end{equation}
for this we consider solutions to the time dependent Schrodinger equation
\begin{eqnarray}
\psi(t) &=& e^{-iHt}\psi(0) \\
&=& e^{-iHt} \sum_{n=1}^d A_n \phi_n \\ \label{timdepcoh}
&=& \sum_{n=1}^d A_n e^{-i\lambda_n t} \phi_n
\end{eqnarray}
where $\{ \phi_n \}$ are the orthonormal eigenstates at time 0 with eigenvalues $\{ \lambda_n \}$ and $A_n = \langle \phi_n \mid \psi(0) \rangle$. Form the autocorrelation function of $\psi(t)$
\begin{equation}\label{autocor}
\mathcal{P}(t) = \langle \psi(0) \mid \psi(t) \rangle
\end{equation}
and Fourier transform to obtain
\begin{eqnarray}
\hat{\mathcal{P}}(\lambda) &=& 2\pi \int_{-\infty}^\infty \mathcal{P}(t) e^{-i\lambda t}dt \\
\label{ftautocor}
&=& \sum_{n=1}^d |A_n|^2 \delta(\lambda - \lambda_n).
\end{eqnarray}

If the Schrodinger equation is integrated in $[0,T]$ with step size $\Delta t$ then the computed spectrum is in the range $[-\pi / \Delta t, \pi / \Delta t]$ with resolution $\Delta \lambda = 2\pi/\Delta t$. In order to capture the full spectrum and avoid aliasing errors the overriding concern for timestep selection is therefore
\begin{equation}
\lambda_{max} - \lambda_{min} < \frac{2\pi}{\Delta t}.
\end{equation}

If an FFT is used to transform the autocorrelation function into the energy domain it is neccessary to sample $\mathcal{P}(t)$ at all $N$ grid points in the time domain. Note however that the number of nonzeros of $\hat{\mathcal{P}}(\lambda)$, $d$, is typically much smaller than $N$. In this situation we may take advantage of new techniques in sparse signal processing known collectively as ``compressed sensing'' allowing us to sample $\mathcal{P}(t)$ at significantly fewer points by replacing the FFT with a convex optimization problem.

\subsection{Compressed sensing}

Generic summary of compressed sensing, Bregman, l1, etc.

\section{Method}

We propose propagation of $\psi(t)$ via a randomized time stepping scheme. Given a division of $[0,T]$ into $N$ points spaced $\Delta t$ apart we draw $M \in \mathcal{O}(d \log(N))$ points uniformly at random where $\psi(t)$ will be evaluated. Denote by $\Delta t_n$ the spacing between the subsamples grid points, our method then proceeds by applying propagators of varying stepsizes to $\psi(t)$ and recording $\mathcal{P}(t_n)$ at each iteration. We finally move into the energy domain with $l1$ minimization.

\begin{algorithmic}
\FOR{$n=1$ to $M$}
\STATE $\psi(t_n) = e^{-iH\Delta t_n}\psi(t_{n-1})$
\STATE $\mathcal{P}(t_n) = \int \psi(t_n)\psi^*(0) dx$
\ENDFOR
\STATE $\hat{\mathcal{P}}(\lambda) = \min \{ |u|_1 s.t. Ru = \mathcal{P} \}$
\end{algorithmic}
where $R$ is an $M \times N$ subsampled DFT matrix. While the $l1$ minimization is less efficient than an FFT we note that since the time-energy conversion is between one dimensional signals the added cost is negligible when compared with the cost of advancing $\psi(t)$.

\subsection{Choice of propagator}
Accurate computation of $e^{-iH\Delta t}$ for varying $\Delta t$ is necessary for successful determination of $\mathcal{P}(t)$. We favor approximating $e^{-iHdt}$ via an expansion in Chebyshev polynomials \cite{Aviv1984} as this method provides sufficient accuracy and robustness to demonstrate our method. Specifically,
$$
e^{-iH\Delta t} = \sum_{k=0}^K a_k \rho_k (-iHdt).
$$
where $\rho_k(\omega) = T_k(-i\omega)$, $\omega \in [-i,i]$ are the complex Chebyshev polynomials. To understand bounds on $K$ we consider the problem of approximating $e^z$ for $z \in [i \lambda_{min} dt, i \lambda_{max} dt]$. Introduce notation
$$
R = \frac{\Delta t}{2}(\lambda_{max} - \lambda_{min})
$$

$$
G = \Delta t \lambda_{min}
$$

$$
\omega = \frac{1}{R}(z - i(R+G)),\omega \in [-i,i]
$$

And write
\begin{eqnarray}
e^{z} & = & e^{i(R+G)}e^{R\omega} \\
& = & e^{i(R+G)}\sum_{k=0}^K C_k J_k(R) \rho_k(\omega)
\end{eqnarray}
where $C_k \in \{1,2 \} $. Note that the Bessel functions of the first kind, $J_k(R)$ vanish for $k>R$ and almost never vanish for $k<R$. We therefore take $K= \alpha R$ with $\alpha >1$ to ensure convergence of the series.

Regrettably, the computational cost (measured by the number of calls to the Hamiltonian) of applying a Chebyshev polynomial expansion scales linearly with respect to both the timestep size and the spectral radius of $H$. We leave development of a propagator with an asymptotically decreasing computational cost per unit timestep to future work.

\section{Numerical Results}

We validate our approach on a finite square well potential

\begin{eqnarray*}
H &=& \frac{-1}{2M}\triangle + V(x) \\
V(x) &=& 2.8(x < L/16) + 2.8(x > L/16) \\ 
x &\in & [-L/2, L/2] \\
L &=& 48\\
M &=& 25\\
Nx &=& 512 \\
\Delta t &=& 0.5 \\
T &=& 1941 \\
\psi(0) &=& e^{-(5(x-0.7))^2}
\end{eqnarray*}

With a final time of 1941 (atomic units?) and $\Delta t = 0.5$ we get 3882 points in out time domain. We downsample by factors of 2,4,6 and 8 and record the relative error in  $\hat{\mathcal{P}}(\lambda)$.


\begin{center}
\begin{tabular}{ l | r  }
  \hline                       
  $\#$ subsampled timesteps & relative error \\ \hline
   1941 & 2.7109e-4 \\
   971 & .0062 \\
   647 & .0138 \\
   486 &  .0234 \\
  \hline  
\end{tabular}
\end{center}


Here we plot $\hat{\mathcal{P}}(\lambda)$ constructed from the full data using an FFT in blue and the $l1$ reconstruction in red. Interestingly, the reconstructed data tends to favor a sparser solution and may in fact be closer to the ground truth.

\includegraphics{t1941.eps}
\includegraphics{t1941_zoom.eps}


\bibliographystyle{plain}
\bibliography{thebib}

\end{document}
